
**`pyproject.toml`**
```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "local-ai-chat"
version = "1.0.0"
description = "A completely offline ChatGPT-like application that runs entirely on your laptop"
authors = [
    {name = "LocalAI Chat Team", email = "hello@example.com"},
]
readme = "README.md"
license = {text = "MIT"}
keywords = ["ai", "chat", "offline", "llm", "local"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
]
requires-python = ">=3.8"
dependencies = [
    "fastapi>=0.104.1",
    "uvicorn>=0.24.0",
    "python-multipart>=0.0.6",
    "llama-cpp-python>=0.2.23",
    "PyPDF2>=3.0.1",
    "pytesseract>=0.3.10",
    "Pillow>=10.1.0",
    "python-dotenv>=1.0.0",
    "aiofiles>=23.2.1",
]







### Model Support
# This application supports all GGUF format models. Some recommended models:

#- Small & Fast: TinyLlama, Phi-2, Qwen-1.8B

#- Balanced: Llama-2-7B, Mistral-7B, DeepSeek-Coder-6.7B

#- Powerful: Llama-2-13B, CodeLlama-13B, Qwen-14B

### System Requirements
#RAM: 8GB+ (16GB recommended for larger models)

#Storage: 2GB+ for models

#OS: Windows, macOS, or Linux

# Python: 3.8+

### Privacy & Security
- ðŸ”’ No data leaves your computer

- ðŸ”’ No API keys required

- ðŸ”’ Complete control over your data

- ðŸ”’ Models run locally on your hardware

### Usage
# Load a Model: Select and load a GGUF model from the sidebar

# Start Chatting: Type your message and press Enter

# Upload Documents: Drag and drop PDFs, images, or text files

# Branch Conversations: Create new conversation branches at any point

# JSON Mode: Constrain responses to specific JSON schemas

### Troubleshooting
# Common Issues
# "No models found": Download models using the download script

# Slow responses: Use smaller quantized models (Q4_K_M or smaller)

# Memory issues: Close other applications or use smaller models

### Getting Help
# Check the console output for detailed error messages. Most issues can be resolved by:

# Ensuring models are in the models/ directory

# Installing all Python dependencies

# Having sufficient RAM and storage

